{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Stephan\\anaconda3\\envs\\chatbot-qa\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin c:\\Users\\Stephan\\anaconda3\\envs\\chatbot-qa\\Lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig, PreTrainedModel, LlamaTokenizer, LlamaForCausalLM, pipeline\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from datasets import load_from_disk\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load hugging-face token\n",
    "load_dotenv()\n",
    "hf_token = os.environ[\"HF_ACCESS_TOKEN\"]\n",
    "\n",
    "#set wandb environment variables\n",
    "os.environ[\"WANDB_ENTITY\"] = \"t_buess\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"chatbot-qa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "finetuned_path = \"./data/models/llama2-fine-tuned\"\n",
    "ft_dataset_filename = \"data/processed/ft_dataset.hf\"\n",
    "train_set_text_field = \"text\"\n",
    "batch_size = 1\n",
    "grad_accumulation_steps = 4\n",
    "optimizer = \"paged_adamw_32bit\"\n",
    "learning_rate = 1e-4\n",
    "max_steps = 100\n",
    "max_seq_length = 4096\n",
    "device_map=\"auto\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\Stephan\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/Stephan/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def formatter(example):\n",
    "    prompt = (\n",
    "        \"Nachfolgend ist eine Frage gestellt mit dem entsprechenden Kontext sowie der passenden Antwort\"\n",
    "        \"Schreibe eine passende Antwort zur Frage und beziehe den Kontext mit hinein\"\n",
    "        \"### Frage:\\n\"\n",
    "        f\"{example['question']}\\n\\n\"\n",
    "        \"### Kontext:\\n\"\n",
    "        f\"{example['context']}\\n\\n\"\n",
    "        \"### Antwort:\\n\"\n",
    "        f\"{example['answers']}\"\n",
    "    )\n",
    "\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "#load train dataset\n",
    "train_dataset = load_from_disk(ft_dataset_filename)\n",
    "\n",
    "#add text column\n",
    "train_dataset = train_dataset.map(formatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\Stephan\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/Stephan/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    token=hf_token,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    token=hf_token,\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\Stephan\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/Stephan/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "train_args = TrainingArguments(\n",
    "    output_dir=finetuned_path + \"/train-out\",\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=grad_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    optim=optimizer,\n",
    "    logging_steps=1,\n",
    "    max_steps=max_steps,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "# LoRA adapters added inside the trainer, prepare_model_for_kbit_training is called here\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=train_set_text_field,\n",
    "    max_seq_length=max_seq_length,\n",
    "    args=train_args\n",
    ")\n",
    "\n",
    "fine_tuning.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\Stephan\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/Stephan/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "fine_tuning.model.save_pretrained(finetuned_path + \"/model\")\n",
    "fine_tuning.tokenizer.save_pretrained(finetuned_path + \"/tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\Stephan\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/Stephan/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def predict(model:PreTrainedModel, tokenizer:LlamaTokenizer, question:str, context:str):\n",
    "    model.eval()\n",
    "    \n",
    "    prompt = (\n",
    "        \"Nachfolgend ist eine Frage gestellt mit dem entsprechenden Kontext\\n\"\n",
    "        \"Schreibe eine passende Antwort zur Frage und beziehe den Kontext mit hinein\\n\\n\"\n",
    "        \"### Frage:\\n\"\n",
    "        f\"{question}\\n\\n\"\n",
    "        \"### Kontext:\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        \"### Antwort:\\n\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda:0\"), attention_mask=inputs[\"attention_mask\"], max_new_tokens=200, pad_token_id=tokenizer.pad_token_id, do_sample=True)\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "question = \"Wer hat python erfunden?\"\n",
    "context = (\n",
    "    \"Die Sprache wurde Anfang der 1990er Jahre von Guido van Rossum am Centrum Wiskunde & Informatica in Amsterdam als Nachfolger für die Programmier-Lehrsprache ABC entwickelt\" \n",
    "    \"und war ursprünglich für das verteilte Betriebssystem Amoeba gedacht.\"\n",
    "    \"Der Name geht nicht, wie das Logo vermuten lässt, auf die gleichnamige Schlangengattung Python zurück, sondern bezog sich ursprünglich auf die englische Komikergruppe Monty Python.\" \n",
    "    \"In der Dokumentation finden sich daher auch einige Anspielungen auf Sketche aus dem Flying Circus. Trotzdem etablierte sich die Assoziation zur Schlange, was sich unter anderem\" \n",
    "    \"in der Programmiersprache Cobra[16] sowie dem Python-Toolkit „Boa“[17] äußert. Die erste Vollversion erschien im Januar 1994 unter der Bezeichnung Python 1.0.\" \n",
    "    \"Gegenüber früheren Versionen wurden einige Konzepte der funktionalen Programmierung implementiert, die allerdings später wieder aufgegeben wurden.[18] Von 1995 bis 2000 erschienen neue Versionen,\"\n",
    "    \"die fortlaufend als Python 1.1, 1.2 etc. bezeichnet wurden.\"\n",
    ")\n",
    "    \n",
    "\n",
    "predict(base_model, tokenizer, question, context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
