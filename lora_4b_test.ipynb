{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig, PreTrainedModel, LlamaTokenizer, StoppingCriteria\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from datasets import load_from_disk\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM, PeftModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load hugging-face token\n",
    "load_dotenv()\n",
    "hf_token = os.environ[\"HF_ACCESS_TOKEN\"]\n",
    "\n",
    "#set wandb environment variables\n",
    "os.environ[\"WANDB_ENTITY\"] = \"t_buess\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"chatbot-qa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "finetuned_path = \"./data/models/llama2-fine-tuned\"\n",
    "ft_dataset_filename = \"data/processed/ft_dataset.hf\"\n",
    "train_set_text_field = \"text\"\n",
    "train_batch_size = 1\n",
    "grad_accumulation_steps = 8\n",
    "optimizer = \"paged_adamw_32bit\"\n",
    "learning_rate = 1e-4\n",
    "max_steps = 200\n",
    "max_seq_length = 4096\n",
    "device_map=\"auto\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.18s/it]\n",
      "c:\\Users\\Stephan\\anaconda3\\envs\\chatbot-qa\\Lib\\site-packages\\transformers\\utils\\hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.12433454005023165\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    token=hf_token,\n",
    ")\n",
    "tokenizer.pad_token = \"</p>\"\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    token=hf_token,\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1\n",
    "\n",
    "base_model:AutoPeftModelForCausalLM = prepare_model_for_kbit_training(base_model)\n",
    "base_model = get_peft_model(base_model, lora_config) # add lora adapters\n",
    "base_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatter(example):\n",
    "    prompt = (\n",
    "        \"Nachfolgend ist eine Frage gestellt mit dem entsprechenden Kontext sowie der passenden Antwort\"\n",
    "        \"Schreibe eine passende Antwort zur Frage und beziehe den Kontext mit hinein\"\n",
    "        \"### Frage:\\n\"\n",
    "        f\"{example['question']}\\n\\n\"\n",
    "        \"### Kontext:\\n\"\n",
    "        f\"{example['context']}\\n\\n\"\n",
    "        \"### Antwort:\\n\"\n",
    "        f\"{example['answers']}{tokenizer.eos_token}\"\n",
    "    )\n",
    "\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "#load train dataset\n",
    "train_dataset = load_from_disk(ft_dataset_filename)\n",
    "\n",
    "#add text column\n",
    "train_dataset = train_dataset.map(formatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = TrainingArguments(\n",
    "    output_dir=finetuned_path + \"/train-out\",\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    gradient_accumulation_steps=grad_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    optim=optimizer,\n",
    "    logging_steps=1,\n",
    "    max_steps=max_steps,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    group_by_length=True\n",
    ")\n",
    "\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=train_set_text_field,\n",
    "    max_seq_length=max_seq_length,\n",
    "    args=train_args\n",
    ")\n",
    "\n",
    "fine_tuning.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/models/llama2-fine-tuned/tokenizer\\\\tokenizer_config.json',\n",
       " './data/models/llama2-fine-tuned/tokenizer\\\\special_tokens_map.json',\n",
       " './data/models/llama2-fine-tuned/tokenizer\\\\tokenizer.model',\n",
       " './data/models/llama2-fine-tuned/tokenizer\\\\added_tokens.json',\n",
       " './data/models/llama2-fine-tuned/tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuning.model.save_pretrained(finetuned_path + \"/model\")\n",
    "fine_tuning.tokenizer.save_pretrained(finetuned_path + \"/tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model:PreTrainedModel, tokenizer:LlamaTokenizer, question:str, context:str):\n",
    "    model.eval()\n",
    "    \n",
    "    prompt = (\n",
    "        \"Nachfolgend ist eine Frage gestellt mit dem entsprechenden Kontext\\n\"\n",
    "        \"Schreibe eine passende Antwort als vollständiger Satz zur Frage und beziehe den Kontext mit hinein\\n\\n\"\n",
    "        \"### Frage:\\n\"\n",
    "        f\"{question}\\n\\n\"\n",
    "        \"### Kontext:\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        \"### Antwort:\\n\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda:0\"), attention_mask=inputs[\"attention_mask\"], max_new_tokens=200, pad_token_id=tokenizer.pad_token_id, do_sample=True)\n",
    "\n",
    "    return tokenizer.decode(outputs[:, inputs[\"input_ids\"].shape[1]:][0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python 1.0'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Was was die erste Version von Python?\"\n",
    "context = (\n",
    "    \"Die Sprache wurde Anfang der 1990er Jahre von Guido van Rossum am Centrum Wiskunde & Informatica in Amsterdam als Nachfolger für die Programmier-Lehrsprache ABC entwickelt\" \n",
    "    \"und war ursprünglich für das verteilte Betriebssystem Amoeba gedacht.\"\n",
    "    \"Der Name geht nicht, wie das Logo vermuten lässt, auf die gleichnamige Schlangengattung Python zurück, sondern bezog sich ursprünglich auf die englische Komikergruppe Monty Python.\" \n",
    "    \"In der Dokumentation finden sich daher auch einige Anspielungen auf Sketche aus dem Flying Circus. Trotzdem etablierte sich die Assoziation zur Schlange, was sich unter anderem\" \n",
    "    \"in der Programmiersprache Cobra[16] sowie dem Python-Toolkit „Boa“[17] äußert. Die erste Vollversion erschien im Januar 1994 unter der Bezeichnung Python 1.0.\" \n",
    "    \"Gegenüber früheren Versionen wurden einige Konzepte der funktionalen Programmierung implementiert, die allerdings später wieder aufgegeben wurden.[18] Von 1995 bis 2000 erschienen neue Versionen,\"\n",
    "    \"die fortlaufend als Python 1.1, 1.2 etc. bezeichnet wurden.\"\n",
    ")\n",
    "\n",
    "print(\"Frage: \", question)\n",
    "print(\"Antwort: \", predict(base_model, tokenizer, question, context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.01s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frage:  Was was die erste Version von Python?\n",
      "Antwort:  Python 1.0\n"
     ]
    }
   ],
   "source": [
    "#load hugging-face token\n",
    "load_dotenv()\n",
    "hf_token = os.environ[\"HF_ACCESS_TOKEN\"]\n",
    "\n",
    "#load lora config from model\n",
    "lora_config = LoraConfig.from_pretrained(finetuned_path + \"/model\") \n",
    "\n",
    "#define bnb config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "ft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    lora_config.base_model_name_or_path, \n",
    "    quantization_config=bnb_config, \n",
    "    device_map=\"cuda:0\"\n",
    ")\n",
    "ft_model = PeftModel.from_pretrained(ft_model, finetuned_path + \"/model\")\n",
    "\n",
    "ft_tokenizer = AutoTokenizer.from_pretrained(finetuned_path + \"/tokenizer\")\n",
    "\n",
    "question = \"Was was die erste Version von Python?\"\n",
    "context = (\n",
    "    \"Die Sprache wurde Anfang der 1990er Jahre von Guido van Rossum am Centrum Wiskunde & Informatica in Amsterdam als Nachfolger für die Programmier-Lehrsprache ABC entwickelt\" \n",
    "    \"und war ursprünglich für das verteilte Betriebssystem Amoeba gedacht.\"\n",
    "    \"Der Name geht nicht, wie das Logo vermuten lässt, auf die gleichnamige Schlangengattung Python zurück, sondern bezog sich ursprünglich auf die englische Komikergruppe Monty Python.\" \n",
    "    \"In der Dokumentation finden sich daher auch einige Anspielungen auf Sketche aus dem Flying Circus. Trotzdem etablierte sich die Assoziation zur Schlange, was sich unter anderem\" \n",
    "    \"in der Programmiersprache Cobra[16] sowie dem Python-Toolkit „Boa“[17] äußert. Die erste Vollversion erschien im Januar 1994 unter der Bezeichnung Python 1.0.\" \n",
    "    \"Gegenüber früheren Versionen wurden einige Konzepte der funktionalen Programmierung implementiert, die allerdings später wieder aufgegeben wurden.[18] Von 1995 bis 2000 erschienen neue Versionen,\"\n",
    "    \"die fortlaufend als Python 1.1, 1.2 etc. bezeichnet wurden.\"\n",
    ")\n",
    "\n",
    "print(\"Frage: \", question)\n",
    "print(\"Antwort: \", predict(ft_model, ft_tokenizer, question, context))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
