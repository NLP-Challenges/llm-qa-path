schema: '2.0'
stages:
  build_corpus:
    cmd: python src/stages/build_corpus.py data/raw/example_context.txt data/processed/corpus.pkl
    deps:
    - path: data/raw/example_context.txt
      hash: md5
      md5: 6f1ec549bb9074a2fe151ca623eca5c8
      size: 7161
    - path: src/stages/build_corpus.py
      hash: md5
      md5: 3b203436718e74c61848b90e16606eb9
      size: 910
    outs:
    - path: data/processed/corpus.pkl
      hash: md5
      md5: c5bff24c94fd2a5433666630fa7bc3f2
      size: 7363
  build_embedder:
    cmd: python src/stages/build_embedder.py "sentence-transformers/distiluse-base-multilingual-cased-v1"
      data/processed/embedder.pkl
    deps:
    - path: src/stages/build_embedder.py
      hash: md5
      md5: 8bdeab160c9b0d681db86bcb4b75670e
      size: 733
    outs:
    - path: data/processed/embedder.pkl
      hash: md5
      md5: 3c548e18ba48e7b7adc532ffda7988d1
      size: 542838819
  build_vectorstore:
    cmd: python src/stages/build_vectorstore.py data/processed/corpus.pkl data/processed/embedder.pkl
      data/processed/chroma
    deps:
    - path: data/processed/corpus.pkl
      hash: md5
      md5: c5bff24c94fd2a5433666630fa7bc3f2
      size: 7363
    - path: data/processed/embedder.pkl
      hash: md5
      md5: 3c548e18ba48e7b7adc532ffda7988d1
      size: 542838819
    - path: src/stages/build_vectorstore.py
      hash: md5
      md5: dd5de97d567f420272e32d6be5b3b118
      size: 1159
    outs:
    - path: data/processed/chroma
      hash: md5
      md5: 71925c72783e79e79aa8b1561f06dfa5.dir
      size: 2400996
      nfiles: 5
  load_vectorstore:
    cmd: python src/stages/load_vectorstore.py data/processed/chroma data/processed/embedder.pkl
      "Wo braucht es einen agenten?"
    deps:
    - path: data/processed/chroma
      hash: md5
      md5: 71925c72783e79e79aa8b1561f06dfa5.dir
      size: 2400996
      nfiles: 5
    - path: data/processed/embedder.pkl
      hash: md5
      md5: 3c548e18ba48e7b7adc532ffda7988d1
      size: 542838819
    - path: src/stages/load_vectorstore.py
      hash: md5
      md5: 9aee0b369d1a0dc2cf96a105a9f11be9
      size: 2006
  build_ft_dataset:
    cmd: python src/stages/build_ft_dataset.py build_tf_dataset data/processed/ft_dataset.hf
    deps:
    - path: src/stages/build_ft_dataset.py
      hash: md5
      md5: 32625a229da82d4e7703b3f647ca0590
      size: 3495
    params:
      params.yaml:
        build_tf_dataset:
          seed: 1234
          frac_original: 0.9
          frag_swapped: 0.1
    outs:
    - path: data/processed/ft_dataset.hf
      hash: md5
      md5: a7b0b1c48b9b4498b0b30345ab7a6102.dir
      size: 15995175
      nfiles: 3
  train_llm:
    cmd: python src/stages/train_llm.py train_llm data/processed/ft_dataset.hf ./data/models/llama2-qa-fine-tuned
    deps:
    - path: data/processed/ft_dataset.hf
      hash: md5
      md5: a7b0b1c48b9b4498b0b30345ab7a6102.dir
      size: 15995175
      nfiles: 3
    - path: src/stages/train_llm.py
      hash: md5
      md5: ebeb6b929f8d89b2cfc23fafe438fecd
      size: 4684
    params:
      params.yaml:
        train_llm:
          wandb_params:
            entity: t_buess
            project: chatbot-qa
          model_params:
            base_model_id: meta-llama/Llama-2-13b-hf
            max_seq_length: 4096
          training_config:
            batch_size: 1
            grad_accumulation_steps: 8
            optimizer: paged_adamw_32bit
            learning_rate: 0.0001
            max_steps: 100
            device_map: auto
            completion_only: true
          LoraConfig:
            r: 6
            lora_alpha: 10
            lora_dropout: 0.1
            bias: none
            task_type: CAUSAL_LM
          BitsAndBytesConfig:
            load_in_4bit: true
            bnb_4bit_use_double_quant: true
            bnb_4bit_quant_type: nf4
            bnb_4bit_compute_dtype: torch.bfloat16
    outs:
    - path: ./data/models/llama2-qa-fine-tuned
      hash: md5
      md5: e409354cb726aceff4b594d8673b41b8.dir
      size: 22063254
      nfiles: 8
  load_llm:
    cmd: python src/stages/load_llm.py load_llm ./data/models/llama2-qa-fine-tuned
      "Auf was spielte Python an?" "Die Sprache wurde Anfang der 1990er Jahre von
      Guido van Rossum am Centrum Wiskunde & Informatica in Amsterdam als Nachfolger
      für die Programmier-Lehrsprache ABC entwickelt und war ursprünglich für das
      verteilte Betriebssystem Amoeba gedacht. Der Name geht nicht, wie das Logo vermuten
      lässt, auf die gleichnamige Schlangengattung Python zurück, sondern bezog sich
      ursprünglich auf die englische Komikergruppe Monty Python. In der Dokumentation
      finden sich daher auch einige Anspielungen auf Sketche aus dem Flying Circus.
      Trotzdem etablierte sich die Assoziation zur Schlange, was sich unter anderem
      in der Programmiersprache Cobra[16] sowie dem Python-Toolkit „Boa“[17] äußert.
      Die erste Vollversion erschien im Januar 1994 unter der Bezeichnung Python 1.0.
      Gegenüber früheren Versionen wurden einige Konzepte der funktionalen Programmierung
      implementiert, die allerdings später wieder aufgegeben wurden.[18] Von 1995
      bis 2000 erschienen neue Versionen, die fortlaufend als Python 1.1, 1.2 etc.
      bezeichnet wurden."
    deps:
    - path: ./data/models/llama2-qa-fine-tuned
      hash: md5
      md5: e409354cb726aceff4b594d8673b41b8.dir
      size: 22063254
      nfiles: 8
    - path: src/stages/load_llm.py
      hash: md5
      md5: aef23d6f0180abae2c64510c90dc672a
      size: 2902
    params:
      params.yaml:
        load_llm:
          BitsAndBytesConfig:
            load_in_4bit: true
            bnb_4bit_use_double_quant: true
            bnb_4bit_quant_type: nf4
            bnb_4bit_compute_dtype: torch.bfloat16
          model_params:
            device_map: auto
            max_new_tokens: 200
            top_k: 4
            penalty_alpha: 0.6
            temperature: 0.4
