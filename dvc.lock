schema: '2.0'
stages:
  build_corpus:
    cmd: python src/stages/build_corpus.py data/raw/spaces data/processed/corpus.jsonl
    deps:
    - path: data/raw/spaces
      hash: md5
      md5: 951bdfeeccb1829be3f0c47c85acc486.dir
      size: 2296980
      nfiles: 5
    - path: src/stages/build_corpus.py
      hash: md5
      md5: 24ec12464570f61ff053265036733552
      size: 4022
    outs:
    - path: data/processed/corpus.jsonl
      hash: md5
      md5: d3b6abdb04a3be0608f2cbf43e2c11c4
      size: 2281331
  build_embedder:
    cmd: python src/stages/build_embedder.py "sentence-transformers/distiluse-base-multilingual-cased-v1"
      data/processed/embedder.pkl
    deps:
    - path: src/stages/build_embedder.py
      hash: md5
      md5: 8bdeab160c9b0d681db86bcb4b75670e
      size: 733
    outs:
    - path: data/processed/embedder.pkl
      hash: md5
      md5: 6f6c0318517b9d518629a8c87eec8154
      size: 542838819
  build_vectorstore:
    cmd: python src/stages/build_vectorstore.py data/processed/corpus.jsonl data/processed/embedder.pkl
      data/processed/chroma
    deps:
    - path: data/processed/corpus.jsonl
      hash: md5
      md5: d3b6abdb04a3be0608f2cbf43e2c11c4
      size: 2281331
    - path: data/processed/embedder.pkl
      hash: md5
      md5: 6f6c0318517b9d518629a8c87eec8154
      size: 542838819
    - path: src/stages/build_vectorstore.py
      hash: md5
      md5: 1c2dfe9f7e34dab5a56bb1e6b12e3bd5
      size: 1598
    outs:
    - path: data/processed/chroma
      hash: md5
      md5: 31fa109a937cb364ad50a7fcefab727a.dir
      size: 24078853
      nfiles: 6
  load_vectorstore:
    cmd: python src/stages/load_vectorstore.py data/processed/chroma data/processed/embedder.pkl
      "Was weisst du zum  Modul Vertiefende Themen der Analysis?" --strategy selfquery
    deps:
    - path: data/processed/chroma
      hash: md5
      md5: 31fa109a937cb364ad50a7fcefab727a.dir
      size: 24078853
      nfiles: 6
    - path: data/processed/embedder.pkl
      hash: md5
      md5: 6f6c0318517b9d518629a8c87eec8154
      size: 542838819
    - path: src/stages/load_vectorstore.py
      hash: md5
      md5: 241968fb40bbfb2ae255ce8cf5049a1d
      size: 3547
  build_ft_dataset:
    cmd: python src/stages/build_ft_dataset.py build_tf_dataset data/processed/ft_dataset.hf
    deps:
    - path: src/stages/build_ft_dataset.py
      hash: md5
      md5: 5d60f46f8c8158176cadac57af2deb48
      size: 3925
    params:
      params.yaml:
        build_tf_dataset:
          seed: 1234
          frac_original: 0.03
          frag_swapped: 0.01
    outs:
    - path: data/processed/ft_dataset.hf
      hash: md5
      md5: a744b6c75b0b4acb10cf9b01cc2fa0ef.dir
      size: 649484
      nfiles: 3
  train_llm:
    cmd: python src/stages/train_llm.py train_llm data/processed/ft_dataset_abstractive.hf
      ./data/models/llama2-qa-fine-tuned
    deps:
    - path: data/processed/ft_dataset_abstractive.hf
      hash: md5
      md5: 288c6502142739e2f230b24fc41f832b.dir
      size: 797714
      nfiles: 3
    - path: src/stages/train_llm.py
      hash: md5
      md5: 91e3b4d7e9dc1f6e82a14a5d93d76d35
      size: 4933
    params:
      params.yaml:
        train_llm:
          wandb_params:
            entity: t_buess
            project: chatbot-qa
          model_params:
            base_model_id: meta-llama/Llama-2-13b-hf
            max_seq_length: 4096
          training_config:
            batch_size: 2
            grad_accumulation_steps: 16
            optimizer: paged_adamw_32bit
            learning_rate: 0.0001
            warmup_steps: 5
            lr_scheduler_type: cosine
            max_steps: 50
            device_map: auto
            completion_only: true
          LoraConfig:
            r: 12
            lora_alpha: 10
            lora_dropout: 0.1
            bias: none
            task_type: CAUSAL_LM
          BitsAndBytesConfig:
            load_in_4bit: true
            bnb_4bit_use_double_quant: true
            bnb_4bit_quant_type: nf4
            bnb_4bit_compute_dtype: torch.bfloat16
          DatasetColumns:
            question: question
            context: context
            answer: answers
    outs:
    - path: ./data/models/llama2-qa-fine-tuned
      hash: md5
      md5: 5e096f53281a9c89616acbb3b12dcdb4.dir
      size: 41724055
      nfiles: 8
  load_llm:
    cmd: python src/stages/load_llm.py load_llm ./data/models/llama2-qa-fine-tuned
      ./data/raw/example_context.json "Welche Form hat bei der PCA normalerweise der
      Datensatz?"
    deps:
    - path: ./data/models/llama2-qa-fine-tuned
      hash: md5
      md5: 5e096f53281a9c89616acbb3b12dcdb4.dir
      size: 41724055
      nfiles: 8
    - path: ./data/raw/example_context.json
      hash: md5
      md5: 048e156563ef379c22e6988b124fa1c6
      size: 1041
    - path: src/stages/load_llm.py
      hash: md5
      md5: 0883b9ebc8d01562f40c4b325e06f8f0
      size: 2922
    params:
      params.yaml:
        load_llm:
          BitsAndBytesConfig:
            load_in_4bit: true
            bnb_4bit_use_double_quant: true
            bnb_4bit_quant_type: nf4
            bnb_4bit_compute_dtype: torch.bfloat16
          model_params:
            device_map: auto
            max_new_tokens: 200
            top_k: 4
            penalty_alpha: 0.6
            temperature: 0.4
  build_ft_dataset_gpt_context_chunking:
    cmd: python src/stages/build_ft_dataset_gpt_context_chunking.py data/processed/ft_dataset.hf
      data/processed/ft_dataset_chunked.hf
    deps:
    - path: data/processed/ft_dataset.hf
      hash: md5
      md5: 3827c12f0bfcd7903ab43f2fcc709e21.dir
      size: 1136578
      nfiles: 3
    - path: src/stages/build_ft_dataset_gpt_context_chunking.py
      hash: md5
      md5: 4ae328263d9956cac501cb670ccee633
      size: 2998
    outs:
    - path: data/processed/ft_dataset_chunked.hf
      hash: md5
      md5: 553e76bab0e03dbc4691ea6cf6fab25d.dir
      size: 20402
      nfiles: 3
  build_ft_dataset_gpt_answer:
    cmd: python src/stages/build_ft_dataset_gpt_answer.py data/processed/ft_dataset_source_added.hf
      data/processed/ft_dataset_abstractive.hf
    deps:
    - path: data/processed/ft_dataset_source_added.hf
      hash: md5
      md5: 6d0684fa7f6ea83526c555e42055c9bc.dir
      size: 707668
      nfiles: 3
    - path: src/stages/build_ft_dataset_gpt_answer.py
      hash: md5
      md5: 32648a912bd3424ede6babe9a0db4503
      size: 4161
    outs:
    - path: data/processed/ft_dataset_abstractive.hf
      hash: md5
      md5: 288c6502142739e2f230b24fc41f832b.dir
      size: 797714
      nfiles: 3
  build_ft_dataset_gpt_context_add_source:
    cmd: python src/stages/build_ft_dataset_gpt_context_add_source.py data/processed/ft_dataset.hf
      data/processed/ft_dataset_source_added.hf
    deps:
    - path: data/processed/ft_dataset.hf
      hash: md5
      md5: a744b6c75b0b4acb10cf9b01cc2fa0ef.dir
      size: 649484
      nfiles: 3
    - path: src/stages/build_ft_dataset_gpt_context_add_source.py
      hash: md5
      md5: d23af7f37763fab4c6175d324b319995
      size: 3015
    outs:
    - path: data/processed/ft_dataset_source_added.hf
      hash: md5
      md5: 6d0684fa7f6ea83526c555e42055c9bc.dir
      size: 707668
      nfiles: 3
