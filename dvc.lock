schema: '2.0'
stages:
  build_corpus:
    cmd: python src/stages/build_corpus.py data/raw/spaces data/processed/corpus.jsonl
    deps:
    - path: data/raw/spaces
      hash: md5
      md5: 951bdfeeccb1829be3f0c47c85acc486.dir
      size: 2296980
      nfiles: 5
    - path: src/stages/build_corpus.py
      hash: md5
      md5: 24ec12464570f61ff053265036733552
      size: 4022
    outs:
    - path: data/processed/corpus.jsonl
      hash: md5
      md5: d3b6abdb04a3be0608f2cbf43e2c11c4
      size: 2281331
  build_embedder:
    cmd: python src/stages/build_embedder.py "sentence-transformers/distiluse-base-multilingual-cased-v1"
      data/processed/embedder.pkl
    deps:
    - path: src/stages/build_embedder.py
      hash: md5
      md5: 8bdeab160c9b0d681db86bcb4b75670e
      size: 733
    outs:
    - path: data/processed/embedder.pkl
      hash: md5
      md5: 6f6c0318517b9d518629a8c87eec8154
      size: 542838819
  build_vectorstore:
    cmd: python src/stages/build_vectorstore.py data/processed/corpus.jsonl data/processed/embedder.pkl
      data/processed/chroma
    deps:
    - path: data/processed/corpus.jsonl
      hash: md5
      md5: d3b6abdb04a3be0608f2cbf43e2c11c4
      size: 2281331
    - path: data/processed/embedder.pkl
      hash: md5
      md5: 6f6c0318517b9d518629a8c87eec8154
      size: 542838819
    - path: src/stages/build_vectorstore.py
      hash: md5
      md5: 1c2dfe9f7e34dab5a56bb1e6b12e3bd5
      size: 1598
    outs:
    - path: data/processed/chroma
      hash: md5
      md5: 31fa109a937cb364ad50a7fcefab727a.dir
      size: 24078853
      nfiles: 6
  load_vectorstore:
    cmd: python src/stages/load_vectorstore.py data/processed/chroma data/processed/embedder.pkl
      "Was weisst du zum  Modul Vertiefende Themen der Analysis?" --strategy selfquery
    deps:
    - path: data/processed/chroma
      hash: md5
      md5: 31fa109a937cb364ad50a7fcefab727a.dir
      size: 24078853
      nfiles: 6
    - path: data/processed/embedder.pkl
      hash: md5
      md5: 6f6c0318517b9d518629a8c87eec8154
      size: 542838819
    - path: src/stages/load_vectorstore.py
      hash: md5
      md5: 241968fb40bbfb2ae255ce8cf5049a1d
      size: 3547
  build_ft_dataset:
    cmd: python src/stages/build_ft_dataset.py build_tf_dataset data/processed/ft_dataset.hf
    deps:
    - path: src/stages/build_ft_dataset.py
      hash: md5
      md5: 5d60f46f8c8158176cadac57af2deb48
      size: 3925
    params:
      params.yaml:
        build_tf_dataset:
          seed: 1234
          frac_original: 0.03
          frag_swapped: 0.01
    outs:
    - path: data/processed/ft_dataset.hf
      hash: md5
      md5: a744b6c75b0b4acb10cf9b01cc2fa0ef.dir
      size: 649484
      nfiles: 3
  train_llm:
    cmd: python src/stages/train_llm.py train_llm data/processed/ft_dataset.hf ./data/models/llama2-qa-fine-tuned
    deps:
    - path: data/processed/ft_dataset.hf
      hash: md5
      md5: a7b0b1c48b9b4498b0b30345ab7a6102.dir
      size: 15995175
      nfiles: 3
    - path: src/stages/train_llm.py
      hash: md5
      md5: e9697d3318d1f6fae4ddc6e766d771b3
      size: 4545
    params:
      params.yaml:
        train_llm:
          wandb_params:
            entity: yvokeller
            project: llm-qa-path
          model_params:
            base_model_id: meta-llama/Llama-2-13b-hf
            max_seq_length: 4096
          training_config:
            batch_size: 1
            grad_accumulation_steps: 8
            optimizer: paged_adamw_32bit
            learning_rate: 0.0001
            max_steps: 100
            device_map: auto
            completion_only: true
          LoraConfig:
            r: 6
            lora_alpha: 10
            lora_dropout: 0.1
            bias: none
            task_type: CAUSAL_LM
          BitsAndBytesConfig:
            load_in_4bit: true
            bnb_4bit_use_double_quant: true
            bnb_4bit_quant_type: nf4
            bnb_4bit_compute_dtype: torch.bfloat16
    outs:
    - path: ./data/models/llama2-qa-fine-tuned
      hash: md5
      md5: e409354cb726aceff4b594d8673b41b8.dir
      size: 22063254
      nfiles: 8
  load_llm:
    cmd: python src/stages/load_llm.py load_llm ./data/models/llama2-qa-fine-tuned
      "Auf was spielte Python an?" "Die Sprache wurde Anfang der 1990er Jahre von
      Guido van Rossum am Centrum Wiskunde & Informatica in Amsterdam als Nachfolger
      für die Programmier-Lehrsprache ABC entwickelt und war ursprünglich für das
      verteilte Betriebssystem Amoeba gedacht. Der Name geht nicht, wie das Logo vermuten
      lässt, auf die gleichnamige Schlangengattung Python zurück, sondern bezog sich
      ursprünglich auf die englische Komikergruppe Monty Python. In der Dokumentation
      finden sich daher auch einige Anspielungen auf Sketche aus dem Flying Circus.
      Trotzdem etablierte sich die Assoziation zur Schlange, was sich unter anderem
      in der Programmiersprache Cobra[16] sowie dem Python-Toolkit „Boa“[17] äußert.
      Die erste Vollversion erschien im Januar 1994 unter der Bezeichnung Python 1.0.
      Gegenüber früheren Versionen wurden einige Konzepte der funktionalen Programmierung
      implementiert, die allerdings später wieder aufgegeben wurden.[18] Von 1995
      bis 2000 erschienen neue Versionen, die fortlaufend als Python 1.1, 1.2 etc.
      bezeichnet wurden."
    deps:
    - path: ./data/models/llama2-qa-fine-tuned
      hash: md5
      md5: e409354cb726aceff4b594d8673b41b8.dir
      size: 22063254
      nfiles: 8
    - path: src/stages/load_llm.py
      hash: md5
      md5: a5fda6423d9d317e13acc0260eeed169
      size: 2818
    params:
      params.yaml:
        load_llm:
          BitsAndBytesConfig:
            load_in_4bit: true
            bnb_4bit_use_double_quant: true
            bnb_4bit_quant_type: nf4
            bnb_4bit_compute_dtype: torch.bfloat16
          model_params:
            device_map: auto
            max_new_tokens: 200
            top_k: 4
            penalty_alpha: 0.6
            temperature: 0.4
  build_ft_dataset_gpt_context_chunking:
    cmd: python src/stages/build_ft_dataset_gpt_context_chunking.py data/processed/ft_dataset.hf
      data/processed/ft_dataset_chunked.hf
    deps:
    - path: data/processed/ft_dataset.hf
      hash: md5
      md5: 3827c12f0bfcd7903ab43f2fcc709e21.dir
      size: 1136578
      nfiles: 3
    - path: src/stages/build_ft_dataset_gpt_context_chunking.py
      hash: md5
      md5: 4ae328263d9956cac501cb670ccee633
      size: 2998
    outs:
    - path: data/processed/ft_dataset_chunked.hf
      hash: md5
      md5: 553e76bab0e03dbc4691ea6cf6fab25d.dir
      size: 20402
      nfiles: 3
  build_ft_dataset_gpt_answer:
    cmd: python src/stages/build_ft_dataset_gpt_answer.py data/processed/ft_dataset_source_added.hf
      data/processed/ft_dataset_abstractive.hf
    deps:
    - path: data/processed/ft_dataset_source_added.hf
      hash: md5
      md5: 6d0684fa7f6ea83526c555e42055c9bc.dir
      size: 707668
      nfiles: 3
    - path: src/stages/build_ft_dataset_gpt_answer.py
      hash: md5
      md5: 32648a912bd3424ede6babe9a0db4503
      size: 4161
    outs:
    - path: data/processed/ft_dataset_abstractive.hf
      hash: md5
      md5: 288c6502142739e2f230b24fc41f832b.dir
      size: 797714
      nfiles: 3
  build_ft_dataset_gpt_context_add_source:
    cmd: python src/stages/build_ft_dataset_gpt_context_add_source.py data/processed/ft_dataset.hf
      data/processed/ft_dataset_source_added.hf
    deps:
    - path: data/processed/ft_dataset.hf
      hash: md5
      md5: a744b6c75b0b4acb10cf9b01cc2fa0ef.dir
      size: 649484
      nfiles: 3
    - path: src/stages/build_ft_dataset_gpt_context_add_source.py
      hash: md5
      md5: d23af7f37763fab4c6175d324b319995
      size: 3015
    outs:
    - path: data/processed/ft_dataset_source_added.hf
      hash: md5
      md5: 6d0684fa7f6ea83526c555e42055c9bc.dir
      size: 707668
      nfiles: 3
