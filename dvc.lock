schema: '2.0'
stages:
  build_corpus:
    cmd: python src/stages/build_corpus.py data/raw/spaces data/processed/corpus.jsonl
    deps:
    - path: data/raw/spaces
      hash: md5
      md5: 951bdfeeccb1829be3f0c47c85acc486.dir
      size: 2296980
      nfiles: 5
    - path: src/stages/build_corpus.py
      hash: md5
      md5: 24ec12464570f61ff053265036733552
      size: 4022
    outs:
    - path: data/processed/corpus.jsonl
      hash: md5
      md5: d3b6abdb04a3be0608f2cbf43e2c11c4
      size: 2281331
  build_embedder:
    cmd: python src/stages/build_embedder.py "sentence-transformers/distiluse-base-multilingual-cased-v1"
      data/processed/embedder.pkl
    deps:
    - path: src/stages/build_embedder.py
      hash: md5
      md5: 8bdeab160c9b0d681db86bcb4b75670e
      size: 733
    outs:
    - path: data/processed/embedder.pkl
      hash: md5
      md5: 6f6c0318517b9d518629a8c87eec8154
      size: 542838819
  build_vectorstore:
    cmd: python src/stages/build_vectorstore.py data/processed/corpus.jsonl data/processed/embedder.pkl
      data/processed/chroma
    deps:
    - path: data/processed/corpus.jsonl
      hash: md5
      md5: d3b6abdb04a3be0608f2cbf43e2c11c4
      size: 2281331
    - path: data/processed/embedder.pkl
      hash: md5
      md5: 6f6c0318517b9d518629a8c87eec8154
      size: 542838819
    - path: src/stages/build_vectorstore.py
      hash: md5
      md5: 1c2dfe9f7e34dab5a56bb1e6b12e3bd5
      size: 1598
    outs:
    - path: data/processed/chroma
      hash: md5
      md5: 31fa109a937cb364ad50a7fcefab727a.dir
      size: 24078853
      nfiles: 6
  load_vectorstore:
    cmd: python src/stages/load_vectorstore.py data/processed/chroma data/processed/embedder.pkl
      "Was weisst du zum  Modul Vertiefende Themen der Analysis?" --strategy selfquery
    deps:
    - path: data/processed/chroma
      hash: md5
      md5: 31fa109a937cb364ad50a7fcefab727a.dir
      size: 24078853
      nfiles: 6
    - path: data/processed/embedder.pkl
      hash: md5
      md5: 6f6c0318517b9d518629a8c87eec8154
      size: 542838819
    - path: src/stages/load_vectorstore.py
      hash: md5
      md5: 241968fb40bbfb2ae255ce8cf5049a1d
      size: 3547
  build_ft_dataset:
    cmd: python src/stages/build_ft_dataset.py build_tf_dataset data/processed/ft_dataset.hf
    deps:
    - path: src/stages/build_ft_dataset.py
      hash: md5
      md5: 9947dd8c5cb9d25e053f23af0b92f5c5
      size: 5451
    params:
      params.yaml:
        build_tf_dataset:
          n_train: 20
          n_val: 6
          n_test: 6
          frac_swapped: 0.2
    outs:
    - path: data/processed/ft_dataset.hf
      hash: md5
      md5: 9fdae7012dde681534dc5ea6f9988c2d.dir
      size: 59507
      nfiles: 3
  train_llm:
    cmd: python src/stages/train_llm.py train_llm data/processed/ft_dataset_abstractive.hf
      ./data/models/llama2-qa-fine-tuned
    deps:
    - path: data/processed/ft_dataset_abstractive.hf
      hash: md5
      md5: 25beb7572f61ec663aaac13c910493df.dir
      size: 123104
      nfiles: 3
    - path: src/stages/train_llm.py
      hash: md5
      md5: 6a05e40bf29f5bd81e1596a657854e7f
      size: 10822
    params:
      params.yaml:
        train_llm:
          hf_hub_repo: nlpchallenges/chatbot-qa-path
          wandb_params:
            entity: t_buess
            project: chatbot-qa
          model_params:
            model_id: flozi00/Llama-2-13b-german-assistant-v7
            pre_train_config:
              use_cache: false
              max_position_embeddings: 4096
            post_train_config:
              use_cache: true
          tokenizer_params:
            tokenizer_id: flozi00/Llama-2-13b-german-assistant-v7
            max_seq_length: 4096
            max_new_tokens: 200
          training_config:
            train_batch_size: 1
            grad_accumulation_steps: 1
            optimizer: paged_adamw_32bit
            learning_rate: 0.0001
            warmup_steps: 0
            logging_steps: 1
            lr_scheduler_type: linear
            num_train_epochs: 1
            device_map: auto
            gradient_checkpointing: true
            completion_only: true
            neftune_noise_alpha: 1
          validation_config:
            val_batch_size: 16
            validate_every_n_steps: 1
            early_stopping_patience: 3
          LoraConfig:
            r: 12
            lora_alpha: 10
            lora_dropout: 0.1
            bias: none
            task_type: CAUSAL_LM
          BitsAndBytesConfig:
            load_in_4bit: true
            bnb_4bit_use_double_quant: true
            bnb_4bit_quant_type: nf4
            bnb_4bit_compute_dtype: torch.bfloat16
          DatasetColumns:
            question: question
            context: sourced_context
            answer: abstractive_answer
            split: split
    outs:
    - path: ./data/models/llama2-qa-fine-tuned
      hash: md5
      md5: 004098aa80de17eb407f850c7766f1ff.dir
      size: 41871028
      nfiles: 9
  load_llm:
    cmd: python src/stages/load_llm.py inference ./data/models/llama2-qa-fine-tuned
      ./data/raw/example_context.json "Was ist DVC?"
    deps:
    - path: ./data/models/llama2-qa-fine-tuned
      hash: md5
      md5: 382381ffa23a6fee58f6b35a1ee3ce63.dir
      size: 41871028
      nfiles: 9
    - path: ./data/raw/example_context.json
      hash: md5
      md5: 048e156563ef379c22e6988b124fa1c6
      size: 1041
    - path: src/stages/load_llm.py
      hash: md5
      md5: 64a38fe9dafea2ad2d3166557a7cf33c
      size: 3348
    params:
      params.yaml:
        inference:
          model_params:
            device_map: auto
          generation_params:
            max_new_tokens: 500
            top_k: 6
            penalty_alpha: 0.6
            temperature: 0.3
          tokenizer_params:
            max_seq_length: 4096
          BitsAndBytesConfig:
            load_in_4bit: true
            bnb_4bit_use_double_quant: true
            bnb_4bit_quant_type: nf4
            bnb_4bit_compute_dtype: torch.bfloat16
  build_ft_dataset_gpt_context_chunking:
    cmd: python src/stages/build_ft_dataset_gpt_context_chunking.py data/processed/ft_dataset.hf
      data/processed/ft_dataset_chunked.hf
    deps:
    - path: data/processed/ft_dataset.hf
      hash: md5
      md5: 3827c12f0bfcd7903ab43f2fcc709e21.dir
      size: 1136578
      nfiles: 3
    - path: src/stages/build_ft_dataset_gpt_context_chunking.py
      hash: md5
      md5: 4ae328263d9956cac501cb670ccee633
      size: 2998
    outs:
    - path: data/processed/ft_dataset_chunked.hf
      hash: md5
      md5: 553e76bab0e03dbc4691ea6cf6fab25d.dir
      size: 20402
      nfiles: 3
  build_ft_dataset_gpt_answer:
    cmd: python src/stages/build_ft_dataset_gpt_answer.py data/processed/ft_dataset_source_added.hf
      data/processed/ft_dataset_abstractive.hf
    deps:
    - path: data/processed/ft_dataset_source_added.hf
      hash: md5
      md5: 097fe9e3043ab12cdf4997a1bab38f2d.dir
      size: 117407
      nfiles: 3
    - path: src/stages/build_ft_dataset_gpt_answer.py
      hash: md5
      md5: 9858ddd0e12b27481460343b7fe4f85d
      size: 4249
    outs:
    - path: data/processed/ft_dataset_abstractive.hf
      hash: md5
      md5: 25beb7572f61ec663aaac13c910493df.dir
      size: 123104
      nfiles: 3
  build_ft_dataset_gpt_context_add_source:
    cmd: python src/stages/build_ft_dataset_gpt_context_add_source.py data/processed/ft_dataset.hf
      data/processed/ft_dataset_source_added.hf
    deps:
    - path: data/processed/ft_dataset.hf
      hash: md5
      md5: 9fdae7012dde681534dc5ea6f9988c2d.dir
      size: 59507
      nfiles: 3
    - path: src/stages/build_ft_dataset_gpt_context_add_source.py
      hash: md5
      md5: c6303409e6cc9cebcb40bdafce3d1af1
      size: 3384
    outs:
    - path: data/processed/ft_dataset_source_added.hf
      hash: md5
      md5: 097fe9e3043ab12cdf4997a1bab38f2d.dir
      size: 117407
      nfiles: 3
  push_model_to_hub:
    cmd: python src/stages/push_model_to_hub.py inference ./data/models/llama2-qa-fine-tuned
      "nlpchallenges/chatbot-qa-path"
    deps:
    - path: ./data/models/llama2-qa-fine-tuned
      hash: md5
      md5: 28757aa1ba3efd9b17c6be6d35e0c799.dir
      size: 41907318
      nfiles: 9
    - path: src/stages/push_model_to_hub.py
      hash: md5
      md5: da3f2395472188a30e646118a7d3cfc2
      size: 1779
    params:
      params.yaml:
        inference:
          model_params:
            device_map: auto
          generation_params:
            max_new_tokens: 500
            top_k: 6
            penalty_alpha: 0.6
            temperature: 0.3
          tokenizer_params:
            max_seq_length: 4096
          BitsAndBytesConfig:
            load_in_4bit: true
            bnb_4bit_use_double_quant: true
            bnb_4bit_quant_type: nf4
            bnb_4bit_compute_dtype: torch.bfloat16
