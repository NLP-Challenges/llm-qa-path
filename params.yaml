build_tf_dataset:
  n_train: 4000
  n_val: 500
  n_test: 200
  frac_swapped: 0.5

train_llm:
  hf_hub_repo: "nlpchallenges/chatbot-qa-path" #if you want to enable hub push -> put repo name here otherwise put null

  wandb_params:
    entity: "t_buess"
    project: "chatbot-qa"

  model_params:
    model_id: "flozi00/Llama-2-13b-german-assistant-v7"

    pre_train_config: #do you want to alter the model config before training? If not set this parameter to null!
      use_cache: False #turn of cache to prevent problems during training!
      max_position_embeddings: 4096 #max positional encoding wrong in this version of llama 2 13b model -> correction to 4096

    post_train_config: #do you want to alter the model config before saving? If not set this parameter to null!
      use_cache: True #turn caching back on again

  tokenizer_params:
    tokenizer_id: "flozi00/Llama-2-13b-german-assistant-v7"
    max_seq_length: 4096
    max_new_tokens: 200 #used for test stage

  training_config:
    training_split_frac: 0.8 # ]0, 1] 1 -> 100% of training data
    train_batch_size: 1
    grad_accumulation_steps: 1
    optimizer: "paged_adamw_32bit"
    learning_rate: 0.0001
    warmup_steps: 0
    logging_steps: 1
    lr_scheduler_type: "linear"
    num_train_epochs: 1
    device_map: "auto"
    gradient_checkpointing: True #gradient checkpointing reduces memory overhead but increases computation overhead
    completion_only: True #only calculate the loss with respect to the completions if true
    neftune_noise_alpha: 1 #if you want to disable NEFTune set this value to null

  validation_config:
    val_batch_size: 16
    validate_every_n_steps: 1 #calculate validation metrics on val split every n steps
    early_stopping_patience: 3 #if model is not getting better within {early_stopping_patience} according to validation metric -> stop training

  LoraConfig:
    r: 12
    lora_alpha: 10
    lora_dropout: 0.1
    bias: "none"
    task_type: "CAUSAL_LM"

  BitsAndBytesConfig:
    load_in_4bit: True
    bnb_4bit_use_double_quant: True
    bnb_4bit_quant_type: "nf4"
    bnb_4bit_compute_dtype: torch.bfloat16

  DatasetColumns:
    question: "question"
    context: "sourced_context"
    answer: "abstractive_answer"
    split: "split" #column where each element is "train", "val" or "test"
    swap_col: "can_be_answered" #boolean column which shows if question can be answered

inference:
  model_params:
    device_map: "auto"

  generation_params:
    max_new_tokens: 500
    top_k: 6
    penalty_alpha: 0.6
    temperature: 0.3

  tokenizer_params:
    max_seq_length: 4096

  BitsAndBytesConfig:
    load_in_4bit: True
    bnb_4bit_use_double_quant: True
    bnb_4bit_quant_type: "nf4"
    bnb_4bit_compute_dtype: torch.bfloat16