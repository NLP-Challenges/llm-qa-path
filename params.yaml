build_tf_dataset:
  seed: 1234
  frac_original: null # if provided here n_original must be null
  frac_swapped: null # if provided here n_swapped must be null
  n_original: 1500 # if provided here frac_original must be null
  n_swapped: 500 # if provided here frac_swapped must be null

train_llm:
  wandb_params:
    entity: "t_buess"
    project: "chatbot-qa"

  model_params:
    base_model_id: "flozi00/Llama-2-13b-german-assistant-v4"
    max_seq_length: 4096

  training_config:
    batch_size: 2
    grad_accumulation_steps: 16
    optimizer: "paged_adamw_32bit"
    learning_rate: 0.0001
    warmup_steps: 5
    logging_steps: 5
    lr_scheduler_type: "cosine"
    num_train_epochs: 2
    device_map: "auto"
    gradient_checkpointing: True #gradient checkpointing reduces memory overhead but increases computation overhead
    completion_only: True 
    neftune_noise_alpha: 2 # if you want to disable NEFTune set this value to null

  LoraConfig:
    r: 12
    lora_alpha: 10
    lora_dropout: 0.1
    bias: "none"
    task_type: "CAUSAL_LM"

  BitsAndBytesConfig:
    load_in_4bit: True
    bnb_4bit_use_double_quant: True
    bnb_4bit_quant_type: "nf4"
    bnb_4bit_compute_dtype: torch.bfloat16

  DatasetColumns:
    question: "question"
    context: "context"
    answer: "answers"

inference:
  BitsAndBytesConfig:
    load_in_4bit: True
    bnb_4bit_use_double_quant: True
    bnb_4bit_quant_type: "nf4"
    bnb_4bit_compute_dtype: torch.bfloat16

  model_params:
    device_map: "auto"
    max_new_tokens: 200
    top_k: 6
    penalty_alpha: 0.6
    temperature: 0.3