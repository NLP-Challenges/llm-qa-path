build_tf_dataset:
  seed: 1234
  frac_original: 0.06
  frag_swapped: 0.01

train_llm:
  wandb_params:
    entity: "yvokeller"
    project: "llm-qa-path"

  model_params:
    base_model_id: "meta-llama/Llama-2-13b-hf" # mistralai/Mistral-7B-v0.1
    max_seq_length: 4096

  training_config:
    batch_size: 1
    grad_accumulation_steps: 8
    optimizer: "paged_adamw_32bit"
    learning_rate: 0.0001
    max_steps: 100
    device_map: "auto"
    completion_only: True

  LoraConfig:
    r: 6
    lora_alpha: 10
    lora_dropout: 0.1
    bias: "none"
    task_type: "CAUSAL_LM"

  BitsAndBytesConfig:
    load_in_4bit: True
    bnb_4bit_use_double_quant: True
    bnb_4bit_quant_type: "nf4"
    bnb_4bit_compute_dtype: torch.bfloat16

load_llm:
  BitsAndBytesConfig:
    load_in_4bit: True
    bnb_4bit_use_double_quant: True
    bnb_4bit_quant_type: "nf4"
    bnb_4bit_compute_dtype: torch.bfloat16

  model_params:
    device_map: "auto"
    max_new_tokens: 200
    top_k: 4
    penalty_alpha: 0.6
    temperature: 0.4