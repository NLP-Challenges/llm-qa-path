{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Stephan\\anaconda3\\envs\\chatbot-qa\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import MistralForCausalLM, LlamaForCausalLM, LongformerForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.07s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MistralConfig {\n",
       "  \"_name_or_path\": \"VAGOsolutions/SauerkrautLM-7b-v1-mistral\",\n",
       "  \"architectures\": [\n",
       "    \"MistralForCausalLM\"\n",
       "  ],\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 14336,\n",
       "  \"max_position_embeddings\": 32768,\n",
       "  \"model_type\": \"mistral\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"sliding_window\": 4096,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.34.0\",\n",
       "  \"use_cache\": false,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MistralForCausalLM.from_pretrained(\"VAGOsolutions/SauerkrautLM-7b-v1-mistral\")\n",
    "model.config.output_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralModel(\n",
       "  (embed_tokens): Embedding(32000, 4096)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x MistralDecoderLayer(\n",
       "      (self_attn): MistralAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (rotary_emb): MistralRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): MistralMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): MistralRMSNorm()\n",
       "      (post_attention_layernorm): MistralRMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): MistralRMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT:\n",
    "\n",
    "https://huggingface.co/docs/transformers/model_doc/mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verwendete Modelle\n",
    "\n",
    "## Modellarchitektur LLama2 13b\n",
    "\n",
    "Ressourcen:\n",
    "- LLaMA explained ([Umar Jamil](https://www.youtube.com/watch?v=Mn_9W1nCFLo))\n",
    "- LLaMA: Open and Efficient Foundation Language Models ([Hugo Touvron et al.](https://arxiv.org/abs/2302.13971))\n",
    "- Llama 2: Open Foundation and Fine-Tuned Chat Models ([Hugo Touvron et al.](https://arxiv.org/abs/2307.09288))\n",
    "\n",
    "![title](img/llama2-architecture.png)\n",
    "\n",
    "## Modellarchitektur Mistral 7b\n",
    "\n",
    "Ressourcen:\n",
    "- Mistral 7B ([Albert Q. Jiang et al.](https://arxiv.org/abs/2310.06825))\n",
    "- Mistral source code ([Huggingface](https://github.com/huggingface/transformers/blob/main/src/transformers/models/mistral/modeling_mistral.py))\n",
    "\n",
    "![title](img/mistral-archtecture.png)\n",
    "\n",
    "## Unterschiede Vorteile / Nachteile\n",
    "\n",
    "Die Modellarchitekturen des LLaMA 2 und Mistral unterscheiden sich kaum.  \n",
    "Die Verwendung der RMSnorm, Aufbau des Feedforward-layers, GQA (nur bei LLaMA 2 34/70b) sowie rotary positional Encoding, sind exakt gleich.  \n",
    "Mistral nutzt im Gegensatz zum LLaMA 2 allerdings zusätzlich eine Sliding-Window-Attention.  \n",
    "Vergleicht man die Parameter des Mistral 7b und LLaMA 2 7b, so fällt auf, dass activation function, hidden size, sowie Anzahl attention heads exakt gleich sind.  \n",
    "\n",
    "Unterscheiden tun sich die Modelle in der Art der Attention: \n",
    "- LLaMA 2 7b: MHA\n",
    "- Mistral 7b: GQA mit SWA\n",
    "\n",
    "und in der Grösse der intermediate size, da diese des Mistral dasjenige der LLaMA 2 7/13b übersteigen.\n",
    "\n",
    "Nach [Bau et al.](https://arxiv.org/abs/2202.05262) befinden sich die factual associations hauptsächlich in den MLP layers.  \n",
    "Mistral nutzt eine grössere intermediate size als die LLaMA 2 7/13b. Möglicherweise können dadurch mehrere factual associations abgespeichert werden.  \n",
    "Dies wäre eine Erklärung weshalb das Mistral in den Benchmarks im Paper von [Albert Q. Jiang et al.](https://arxiv.org/abs/2310.06825) besser performt als die genannten LLaMA 2 modelle.  \n",
    "\n",
    "Betrachten wir die Anzahl MLP parameter so können wir folgendes beobachten:\n",
    "\n",
    "$$\n",
    "MLP\\_PARAMS_{Mistral\\ 7b} = 32*3*4096*14336 = 5637144576\n",
    "$$\n",
    "\n",
    "$$\n",
    "MLP\\_PARAMS_{LLaMA2\\ 7b} = 32*3*4096*11008 = 4328521728\n",
    "$$\n",
    "\n",
    "$$\n",
    "MLP\\_PARAMS_{LLaMA2\\ 13b} = 40*3*5120*13824 = 8493465600\n",
    "$$\n",
    "\n",
    "Das Mistral 7b besitzt mehr Parameter auf den MLPs als das LLaMA 2 7b.  \n",
    "Somit kann das Mistral möglicherweise mehr Informationen aufnehmen als das LLaMA 2 7b.  \n",
    "Das LLaMA 2 13b besitzt allerdings insgesamt mehr Parameter in den MLPs, weshalb man erwarten könnte, dass dies besser performen muss als das Mistral.  \n",
    "Es könnte sein, dass die MLPs in den early Layers ähnlich wie bei CNNs auf kleinere Strukturen und MLPs in den late Layers auf grössere Konzepte achten.  \n",
    "Dies könnte eine Erklärung dafür sein, dass das Mistral trotz weniger MLP Parameter laut den Autoren besser performt als das LLaMA 13b.  \n",
    "Die bessere performance allerdings auch von den Benchmarks abhängen, da das LLaMA nicht eine solch grosse Kontext-size besitzen.  \n",
    "\n",
    "Das Mistral hat durch die Verwendung der Sliding-window-attention definitiv einen Vorteil gegenüber dem LLaMA aufgrund der grösseren Kontext-size.  \n",
    "Dadurch kann Mistral längere Texte als 4096 Tokens verarbeiten.  \n",
    "Die Grouped-query-attention ist ein Tradeoff zwischen Genauigkeit und Geschwindigkeit des Modells, weshalb sowohl LLaMA als auch die Mistral diese nutzen.  \n",
    "\n",
    "TODO: Werden die Factual associations möglicherweise besser über die MLPs verteilt durch die nutzung der SWA?\n",
    "\n",
    "## Weshalb wurden diese Modelle verwendet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodik\n",
    "\n",
    "## Ziel\n",
    "\n",
    "## Finetuning\n",
    "\n",
    "### Datenset\n",
    "\n",
    "### Training\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "### Quantitative Analyse\n",
    "\n",
    "### Qualitative Analyse\n",
    "\n",
    "#### Protokoll\n",
    "\n",
    "### Fehleranalyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eda Datenset\n",
    "\n",
    "Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluationsmetriken\n",
    "\n",
    "## Quantitative Analyse\n",
    "\n",
    "Bleu / Rouge\n",
    "\n",
    "## Qualitative Analyse\n",
    "\n",
    "Factuality / Coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitiative Analyse\n",
    "\n",
    "Eval loss / Bleu / Rouge\n",
    "\n",
    "Wandb report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Analyse\n",
    "\n",
    "Spaces Kontext / Frage / Anwort / Factuality / Coverage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fehleranalyse / Korrekturversuch\n",
    "\n",
    "Wo wurden Fehler gemacht?\n",
    "\n",
    "ROME\n",
    "\n",
    "Attention scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
