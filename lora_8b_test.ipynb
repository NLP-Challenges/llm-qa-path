{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Stephan\\anaconda3\\envs\\chatbot-qa\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin c:\\Users\\Stephan\\anaconda3\\envs\\chatbot-qa\\Lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
     ]
    }
   ],
   "source": [
    "#based on https://www.youtube.com/watch?v=eTieetk2dSw, https://www.philschmid.de/fine-tune-flan-t5-peft, https://medium.com/@ud.chandra/instruction-fine-tuning-llama-2-with-pefts-qlora-method-d6a801ebb19, \n",
    "\n",
    "from transformers import AutoModelForCausalLM, PreTrainedModel, AutoTokenizer, TrainingArguments\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from datasets import load_from_disk\n",
    "from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set wandb environment variables\n",
    "os.environ[\"WANDB_ENTITY\"] = \"t_buess\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"chatbot-qa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load hugging-face token\n",
    "load_dotenv()\n",
    "hf_token = os.environ[\"HF_ACCESS_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the arguments\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "output_dir = \"data/models/llama2-test\"\n",
    "ft_dataset_filename = \"data/processed/ft_dataset.hf\"\n",
    "max_seq_length = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatter(example):\n",
    "    prompt = (\n",
    "        \"Nachfolgend ist eine Frage gestellt mit dem entsprechenden Kontext sowie der passenden Antwort\"\n",
    "        \"Schreibe eine passende Antwort zur Frage und beziehe den Kontext mit hinein\"\n",
    "        \"### Frage:\\n\"\n",
    "        f\"{example['question']}\\n\\n\"\n",
    "        \"### Kontext:\\n\"\n",
    "        f\"{example['context']}\\n\\n\"\n",
    "        \"### Antwort:\\n\"\n",
    "        f\"{example['answers']}\"\n",
    "    )\n",
    "\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "#load train dataset\n",
    "train_dataset = load_from_disk(ft_dataset_filename)\n",
    "\n",
    "#add text column\n",
    "train_dataset = train_dataset.map(formatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Was wird beim Unüberwachten Lernen gemacht?\"\n",
    "context = (\n",
    "    \"Unüberwachtes Lernen\"\n",
    "    \"→ Hauptartikel: Unüberwachtes Lernen\"\n",
    "    \"Der Algorithmus erzeugt für eine gegebene Menge von Eingaben ein statistisches Modell, das die Eingaben beschreibt und erkannte Kategorien und Zusammenhänge enthält und somit Vorhersagen ermöglicht. Dabei gibt es Clustering-Verfahren, die die Daten in mehrere Kategorien einteilen, die sich durch charakteristische Muster voneinander unterscheiden. Das Netz erstellt somit selbständig Klassifikatoren, nach denen es die Eingabemuster einteilt. Ein wichtiger Algorithmus in diesem Zusammenhang ist der EM-Algorithmus, der iterativ die Parameter eines Modells so festlegt, dass es die gesehenen Daten optimal erklärt. Er legt dabei das Vorhandensein nicht beobachtbarer Kategorien zugrunde und schätzt abwechselnd die Zugehörigkeit der Daten zu einer der Kategorien und die Parameter, die die Kategorien ausmachen. Eine Anwendung des EM-Algorithmus findet sich beispielsweise in den Hidden Markov Models (HMMs). Andere Methoden des unüberwachten Lernens, z. B. Hauptkomponentenanalyse, verzichten auf die Kategorisierung. Sie zielen darauf ab, die beobachteten Daten in eine einfachere Repräsentation zu übersetzen, die sie trotz drastisch reduzierter Information möglichst genau wiedergibt.\"\n",
    "    \"Des Weiteren unterscheidet man zwischen Batch-Lernen, bei dem alle Eingabe/Ausgabe-Paare gleichzeitig vorhanden sind, und kontinuierlichem (sequentiellem) Lernen, bei dem sich die Struktur des Netzes zeitlich versetzt entwickelt.\"\n",
    "    \"Außerdem unterscheidet man zwischen Off-line-Lernen, bei dem alle Daten gespeichert sind und somit wiederholbar zugreifbar sind, und On-line-Lernen, bei dem die Daten nach einmaligem Ausführen und Anpassen der Gewichte verloren gehen. Batch Training ist immer off-line, On-line-Training ist immer inkrementell. Inkrementelles Lernen kann jedoch on-line oder off-line erfolgen.[8]\"\n",
    ")\n",
    "\n",
    "prompt = (\n",
    "        \"Nachfolgend ist eine Frage gestellt mit dem entsprechenden Kontext\\n\"\n",
    "        \"Schreibe eine passende Antwort zur Frage und beziehe den Kontext mit hinein\\n\\n\"\n",
    "        \"### Frage:\\n\"\n",
    "        f\"{query}\\n\\n\"\n",
    "        \"### Kontext:\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        \"### Antwort:\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# before fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Stephan\\ml\\npr-challenges&chatbot\\llm-qa-path\\lora_8b_test.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Stephan/ml/npr-challenges%26chatbot/llm-qa-path/lora_8b_test.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model:PreTrainedModel \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Stephan/ml/npr-challenges%26chatbot/llm-qa-path/lora_8b_test.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     model_name,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Stephan/ml/npr-challenges%26chatbot/llm-qa-path/lora_8b_test.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     load_in_8bit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Stephan/ml/npr-challenges%26chatbot/llm-qa-path/lora_8b_test.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcuda:0\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Stephan/ml/npr-challenges%26chatbot/llm-qa-path/lora_8b_test.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     token\u001b[39m=\u001b[39;49mhf_token\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Stephan/ml/npr-challenges%26chatbot/llm-qa-path/lora_8b_test.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Stephan/ml/npr-challenges%26chatbot/llm-qa-path/lora_8b_test.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Stephan/ml/npr-challenges%26chatbot/llm-qa-path/lora_8b_test.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     model_name,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stephan/ml/npr-challenges%26chatbot/llm-qa-path/lora_8b_test.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     token\u001b[39m=\u001b[39mhf_token\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stephan/ml/npr-challenges%26chatbot/llm-qa-path/lora_8b_test.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stephan/ml/npr-challenges%26chatbot/llm-qa-path/lora_8b_test.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m tokenizer\u001b[39m.\u001b[39mpad_token \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m[PAD]\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Stephan\\anaconda3\\envs\\chatbot-qa\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:565\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    564\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 565\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m    566\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    567\u001b[0m     )\n\u001b[0;32m    568\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    569\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    571\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Stephan\\anaconda3\\envs\\chatbot-qa\\Lib\\site-packages\\transformers\\modeling_utils.py:3307\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3297\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3298\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[0;32m   3300\u001b[0m     (\n\u001b[0;32m   3301\u001b[0m         model,\n\u001b[0;32m   3302\u001b[0m         missing_keys,\n\u001b[0;32m   3303\u001b[0m         unexpected_keys,\n\u001b[0;32m   3304\u001b[0m         mismatched_keys,\n\u001b[0;32m   3305\u001b[0m         offload_index,\n\u001b[0;32m   3306\u001b[0m         error_msgs,\n\u001b[1;32m-> 3307\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[0;32m   3308\u001b[0m         model,\n\u001b[0;32m   3309\u001b[0m         state_dict,\n\u001b[0;32m   3310\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[0;32m   3311\u001b[0m         resolved_archive_file,\n\u001b[0;32m   3312\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m   3313\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[0;32m   3314\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[0;32m   3315\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[0;32m   3316\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[0;32m   3317\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[0;32m   3318\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[0;32m   3319\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[0;32m   3320\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[0;32m   3321\u001b[0m         is_quantized\u001b[39m=\u001b[39;49m(\u001b[39mgetattr\u001b[39;49m(model, \u001b[39m\"\u001b[39;49m\u001b[39mquantization_method\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m) \u001b[39m==\u001b[39;49m QuantizationMethod\u001b[39m.\u001b[39;49mBITS_AND_BYTES),\n\u001b[0;32m   3322\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[0;32m   3323\u001b[0m     )\n\u001b[0;32m   3325\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_4bit \u001b[39m=\u001b[39m load_in_4bit\n\u001b[0;32m   3326\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_8bit \u001b[39m=\u001b[39m load_in_8bit\n",
      "File \u001b[1;32mc:\\Users\\Stephan\\anaconda3\\envs\\chatbot-qa\\Lib\\site-packages\\transformers\\modeling_utils.py:3695\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[0;32m   3693\u001b[0m \u001b[39mif\u001b[39;00m low_cpu_mem_usage:\n\u001b[0;32m   3694\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fsdp_enabled() \u001b[39mor\u001b[39;00m is_fsdp_enabled_and_dist_rank_0():\n\u001b[1;32m-> 3695\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[39m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[0;32m   3696\u001b[0m             model_to_load,\n\u001b[0;32m   3697\u001b[0m             state_dict,\n\u001b[0;32m   3698\u001b[0m             loaded_keys,\n\u001b[0;32m   3699\u001b[0m             start_prefix,\n\u001b[0;32m   3700\u001b[0m             expected_keys,\n\u001b[0;32m   3701\u001b[0m             device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[0;32m   3702\u001b[0m             offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[0;32m   3703\u001b[0m             offload_index\u001b[39m=\u001b[39;49moffload_index,\n\u001b[0;32m   3704\u001b[0m             state_dict_folder\u001b[39m=\u001b[39;49mstate_dict_folder,\n\u001b[0;32m   3705\u001b[0m             state_dict_index\u001b[39m=\u001b[39;49mstate_dict_index,\n\u001b[0;32m   3706\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   3707\u001b[0m             is_quantized\u001b[39m=\u001b[39;49mis_quantized,\n\u001b[0;32m   3708\u001b[0m             is_safetensors\u001b[39m=\u001b[39;49mis_safetensors,\n\u001b[0;32m   3709\u001b[0m             keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[0;32m   3710\u001b[0m         )\n\u001b[0;32m   3711\u001b[0m         error_msgs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_error_msgs\n\u001b[0;32m   3712\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Stephan\\anaconda3\\envs\\chatbot-qa\\Lib\\site-packages\\transformers\\modeling_utils.py:749\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[1;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, is_quantized, is_safetensors, keep_in_fp32_modules)\u001b[0m\n\u001b[0;32m    746\u001b[0m             fp16_statistics \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    748\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mSCB\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m param_name:\n\u001b[1;32m--> 749\u001b[0m             set_module_quantized_tensor_to_device(\n\u001b[0;32m    750\u001b[0m                 model, param_name, param_device, value\u001b[39m=\u001b[39;49mparam, fp16_statistics\u001b[39m=\u001b[39;49mfp16_statistics\n\u001b[0;32m    751\u001b[0m             )\n\u001b[0;32m    753\u001b[0m \u001b[39mreturn\u001b[39;00m error_msgs, offload_index, state_dict_index\n",
      "File \u001b[1;32mc:\\Users\\Stephan\\anaconda3\\envs\\chatbot-qa\\Lib\\site-packages\\transformers\\integrations\\bitsandbytes.py:96\u001b[0m, in \u001b[0;36mset_module_quantized_tensor_to_device\u001b[1;34m(module, tensor_name, device, value, fp16_statistics)\u001b[0m\n\u001b[0;32m     94\u001b[0m kwargs \u001b[39m=\u001b[39m old_value\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\n\u001b[0;32m     95\u001b[0m \u001b[39mif\u001b[39;00m is_8bit:\n\u001b[1;32m---> 96\u001b[0m     new_value \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mInt8Params(new_value, requires_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m     97\u001b[0m \u001b[39melif\u001b[39;00m is_4bit:\n\u001b[0;32m     98\u001b[0m     new_value \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mParams4bit(new_value, requires_grad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\Stephan\\anaconda3\\envs\\chatbot-qa\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:333\u001b[0m, in \u001b[0;36mInt8Params.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    324\u001b[0m device, dtype, non_blocking, convert_to_format \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39m_parse_to(\n\u001b[0;32m    325\u001b[0m     \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    326\u001b[0m )\n\u001b[0;32m    328\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    329\u001b[0m     device \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    330\u001b[0m     \u001b[39mand\u001b[39;00m device\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    331\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    332\u001b[0m ):\n\u001b[1;32m--> 333\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcuda(device)\n\u001b[0;32m    334\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    335\u001b[0m     new_param \u001b[39m=\u001b[39m Int8Params(\n\u001b[0;32m    336\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mto(\n\u001b[0;32m    337\u001b[0m             device\u001b[39m=\u001b[39mdevice, dtype\u001b[39m=\u001b[39mdtype, non_blocking\u001b[39m=\u001b[39mnon_blocking\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    340\u001b[0m         has_fp16_weights\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_fp16_weights,\n\u001b[0;32m    341\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Stephan\\anaconda3\\envs\\chatbot-qa\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:296\u001b[0m, in \u001b[0;36mInt8Params.cuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    292\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcuda(device)\n\u001b[0;32m    293\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    294\u001b[0m     \u001b[39m# we store the 8-bit rows-major weight\u001b[39;00m\n\u001b[0;32m    295\u001b[0m     \u001b[39m# we convert this weight to the turning/ampere weight during the first inference pass\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m     B \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mcontiguous()\u001b[39m.\u001b[39;49mhalf()\u001b[39m.\u001b[39;49mcuda(device)\n\u001b[0;32m    297\u001b[0m     CB, CBt, SCB, SCBt, coo_tensorB \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdouble_quant(B)\n\u001b[0;32m    298\u001b[0m     \u001b[39mdel\u001b[39;00m CBt\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model:PreTrainedModel = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"cuda:0\",\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token=hf_token\n",
    ")\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model.eval()\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda:0\"), attention_mask=inputs[\"attention_mask\"], max_new_tokens=100, pad_token_id=tokenizer.pad_token_id, do_sample=True)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "del model\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inspiriert von (https://www.philschmid.de/fine-tune-flan-t5-peft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.12433454005023165\n"
     ]
    }
   ],
   "source": [
    "model:PreTrainedModel = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"cuda:0\",\n",
    "    token=hf_token\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token=hf_token\n",
    ")\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbuesst1\u001b[0m (\u001b[33mt_buess\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Stephan\\ml\\npr-challenges&chatbot\\llm-qa-path\\wandb\\run-20231017_193826-i2n1qpn7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/t_buess/chatbot-qa/runs/i2n1qpn7' target=\"_blank\">robust-donkey-19</a></strong> to <a href='https://wandb.ai/t_buess/chatbot-qa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/t_buess/chatbot-qa' target=\"_blank\">https://wandb.ai/t_buess/chatbot-qa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/t_buess/chatbot-qa/runs/i2n1qpn7' target=\"_blank\">https://wandb.ai/t_buess/chatbot-qa/runs/i2n1qpn7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "c:\\Users\\Stephan\\anaconda3\\envs\\chatbot-qa\\Lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Stephan\\anaconda3\\envs\\chatbot-qa\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  0%|          | 1/500 [00:25<3:30:51, 25.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7295, 'learning_rate': 0.0001996, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/500 [00:42<2:51:14, 20.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6986, 'learning_rate': 0.00019920000000000002, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/500 [01:01<2:45:45, 20.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7069, 'learning_rate': 0.0001988, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/500 [01:21<2:43:09, 19.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6416, 'learning_rate': 0.0001984, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/500 [01:41<2:45:19, 20.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6745, 'learning_rate': 0.00019800000000000002, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/500 [02:02<2:47:30, 20.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5904, 'learning_rate': 0.0001976, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 7/500 [02:23<2:47:38, 20.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6497, 'learning_rate': 0.0001972, 'epoch': 0.01}\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"data/processed/training-output\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=1,\n",
    "    max_steps=400,\n",
    "    report_to=\"wandb\",\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/models/llama2-test/tokenizer\\\\tokenizer_config.json',\n",
       " 'data/models/llama2-test/tokenizer\\\\special_tokens_map.json',\n",
       " 'data/models/llama2-test/tokenizer\\\\tokenizer.model',\n",
       " 'data/models/llama2-test/tokenizer\\\\added_tokens.json',\n",
       " 'data/models/llama2-test/tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save model and tokenizer\n",
    "trainer.model.save_pretrained(f\"{output_dir}/model\")\n",
    "trainer.tokenizer.save_pretrained(f\"{output_dir}/tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# after finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nachfolgend ist eine Frage gestellt mit dem entsprechenden KontextSchreibe eine passende Antwort zur Frage und beziehe den Kontext mit hinein### Frage:\n",
      "Was wird beim Unüberwachten Lernen gemacht?\n",
      "\n",
      "### Kontext:\n",
      "Unüberwachtes Lernen→ Hauptartikel: Unüberwachtes LernenDer Algorithmus erzeugt für eine gegebene Menge von Eingaben ein statistisches Modell, das die Eingaben beschreibt und erkannte Kategorien und Zusammenhänge enthält und somit Vorhersagen ermöglicht. Dabei gibt es Clustering-Verfahren, die die Daten in mehrere Kategorien einteilen, die sich durch charakteristische Muster voneinander unterscheiden. Das Netz erstellt somit selbständig Klassifikatoren, nach denen es die Eingabemuster einteilt. Ein wichtiger Algorithmus in diesem Zusammenhang ist der EM-Algorithmus, der iterativ die Parameter eines Modells so festlegt, dass es die gesehenen Daten optimal erklärt. Er legt dabei das Vorhandensein nicht beobachtbarer Kategorien zugrunde und schätzt abwechselnd die Zugehörigkeit der Daten zu einer der Kategorien und die Parameter, die die Kategorien ausmachen. Eine Anwendung des EM-Algorithmus findet sich beispielsweise in den Hidden Markov Models (HMMs). Andere Methoden des unüberwachten Lernens, z. B. Hauptkomponentenanalyse, verzichten auf die Kategorisierung. Sie zielen darauf ab, die beobachteten Daten in eine einfachere Repräsentation zu übersetzen, die sie trotz drastisch reduzierter Information möglichst genau wiedergibt.Des Weiteren unterscheidet man zwischen Batch-Lernen, bei dem alle Eingabe/Ausgabe-Paare gleichzeitig vorhanden sind, und kontinuierlichem (sequentiellem) Lernen, bei dem sich die Struktur des Netzes zeitlich versetzt entwickelt.Außerdem unterscheidet man zwischen Off-line-Lernen, bei dem alle Daten gespeichert sind und somit wiederholbar zugreifbar sind, und On-line-Lernen, bei dem die Daten nach einmaligem Ausführen und Anpassen der Gewichte verloren gehen. Batch Training ist immer off-line, On-line-Training ist immer inkrementell. Inkrementelles Lernen kann jedoch on-line oder off-line erfolgen.[8]\n",
      "\n",
      "### Antwort:\n",
      "Die Eingaben werden mit Hilfe von Clustering-Verfahren in mehrere Kategorien eingeteilt.\n",
      "\n",
      "### Begründung:\n",
      "Unüberwachtes Lernen→ Hauptartikel: Unüberwachtes LernenDer Algorithmus erzeugt für eine gegebene Menge von Eingaben ein statistisches Modell, das die Eingaben beschreibt und erkannte Kategorien und Zusammenhänge enthält und somit Vorhersagen\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda:0\"), attention_mask=inputs[\"attention_mask\"], max_new_tokens=100, pad_token_id=tokenizer.pad_token_id, do_sample=True)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Möglicherweise besser?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model from storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nachfolgend ist eine Frage gestellt mit dem entsprechenden Kontext\n",
      "Schreibe eine passende Antwort zur Frage und beziehe den Kontext mit hinein\n",
      "\n",
      "### Frage:\n",
      "Was wird beim Unüberwachten Lernen gemacht?\n",
      "\n",
      "### Kontext:\n",
      "Unüberwachtes Lernen→ Hauptartikel: Unüberwachtes LernenDer Algorithmus erzeugt für eine gegebene Menge von Eingaben ein statistisches Modell, das die Eingaben beschreibt und erkannte Kategorien und Zusammenhänge enthält und somit Vorhersagen ermöglicht. Dabei gibt es Clustering-Verfahren, die die Daten in mehrere Kategorien einteilen, die sich durch charakteristische Muster voneinander unterscheiden. Das Netz erstellt somit selbständig Klassifikatoren, nach denen es die Eingabemuster einteilt. Ein wichtiger Algorithmus in diesem Zusammenhang ist der EM-Algorithmus, der iterativ die Parameter eines Modells so festlegt, dass es die gesehenen Daten optimal erklärt. Er legt dabei das Vorhandensein nicht beobachtbarer Kategorien zugrunde und schätzt abwechselnd die Zugehörigkeit der Daten zu einer der Kategorien und die Parameter, die die Kategorien ausmachen. Eine Anwendung des EM-Algorithmus findet sich beispielsweise in den Hidden Markov Models (HMMs). Andere Methoden des unüberwachten Lernens, z. B. Hauptkomponentenanalyse, verzichten auf die Kategorisierung. Sie zielen darauf ab, die beobachteten Daten in eine einfachere Repräsentation zu übersetzen, die sie trotz drastisch reduzierter Information möglichst genau wiedergibt.Des Weiteren unterscheidet man zwischen Batch-Lernen, bei dem alle Eingabe/Ausgabe-Paare gleichzeitig vorhanden sind, und kontinuierlichem (sequentiellem) Lernen, bei dem sich die Struktur des Netzes zeitlich versetzt entwickelt.Außerdem unterscheidet man zwischen Off-line-Lernen, bei dem alle Daten gespeichert sind und somit wiederholbar zugreifbar sind, und On-line-Lernen, bei dem die Daten nach einmaligem Ausführen und Anpassen der Gewichte verloren gehen. Batch Training ist immer off-line, On-line-Training ist immer inkrementell. Inkrementelles Lernen kann jedoch on-line oder off-line erfolgen.[8]\n",
      "\n",
      "### Antwort:\n",
      "Unüberwachtes Lernen\n",
      "\n",
      "### Frage:\n",
      "Was wird beim Unüberwachten Lernen gemacht?\n",
      "\n",
      "### Kontext:\n",
      "Unüberwachtes Lernen→ Hauptartikel: Unüberwachtes LernenDer Algorithmus erzeugt für eine gegebene Menge von Eingaben ein statistisches Modell, das die Eingaben beschreibt und erkannte Kategorien und Zusammenhänge enthält und som\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig.from_pretrained(f\"{output_dir}/model\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"{output_dir}/tokenizer\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    lora_config.base_model_name_or_path, \n",
    "    load_in_8bit=True,\n",
    "    device_map=\"cuda:0\",\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda:0\"), attention_mask=inputs[\"attention_mask\"], max_new_tokens=100, pad_token_id=tokenizer.pad_token_id, do_sample=True)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
